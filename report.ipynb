{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6ac65f30-3c41-4079-8db5-430ac346ce7d",
   "metadata": {},
   "source": [
    "# <b> CSCI 580 Artifical Intelligence: Final Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551017d2-f52b-45b9-9a68-c861e0b1bb9b",
   "metadata": {},
   "source": [
    "This report documents our model for handwritten digit recognition. Specifically, it will cover various attributes of our model and its overall performance. The people in our team who worked on the project are <b> Harsh Sharma, Marco U Calderon, Luis Manuel Melgoza, and Kamaldeep Singh <b>\n",
    "\n",
    "Here is a link to a GitHub repository including all relevant materials:  (https://github.com/HarshTheSharma/Final580)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd13b2b3-3693-444b-a532-7991baa46d98",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c701031b-baa8-46ab-afbd-0c2b66a0e7f5",
   "metadata": {},
   "source": [
    "## <b> Our Model's Framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7fe76-0608-4d87-b0af-9da0b3e323c8",
   "metadata": {},
   "source": [
    "<p> A high-level framework for our solution involves using a Multilayer Perceptron (MLP), which is a fully connected feedforward neural network. This framework is implemented using PyTorch and is designed to classify 28 x 28 grayscale handwritten digit images from the MNIST and team-collected datasets.\n",
    "\n",
    "Regarding details about our Neural Net architecture, we have exactly four linear layers: three hidden layers and one output layer. Our model starts with hundreds of neurons that gradually decrease in number as we progress through each layer. We end up with ten neurons in our output layer, corresponding to the 10 digit classes. The total number of weights excluding Batch Norm parameters is around 566,528. Furthermore, our model also uses the Rectified Linear Unit (ReLU) activation function after each hidden layerâ€™s batch normalization, which stabilizes training alongside dropout in the first two hidden layers to avoid overfitting.\n",
    "\n",
    "In addition to our Neural Net architecture, our model is driven by carefully tuned hyperparameters. The model uses the Adam optimizer to update weights with the standard cross-entropy loss function for multi-class classification. Its initial learning rate is set to 0.0005, with L2 regularization (weight decay) of 0.0001 to reduce overfitting. Additionally, a Reduce Learning Rate on Plateau is included, which halves the learning rate if the validation loss plateaus for consecutive epochs. Speaking of epochs, we are doing 100 epochs on the MNIST dataset, and handwritten digits made by 10 groups with 4 members each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff2434f-7352-43cd-b24e-dc1306b4f3e5",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7b8240-1ff9-4918-9838-c2b7df230421",
   "metadata": {},
   "source": [
    "## <b> Detailed Aspects of Our Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f6bd28-e9b1-4b3f-b7d7-6d8b81e33cf4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
